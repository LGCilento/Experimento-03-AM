{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one-class-classification-algorithms-kc2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmKJKqgkPnzDRFZ9toX7lX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LGCilento/Experimento-03-AM/blob/master/one_class_classification_algorithms_kc2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXG0FaBc6kTk"
      },
      "source": [
        "<h1><b>One-class-classification-algorithms</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2lePh896phK"
      },
      "source": [
        "<h2>Data-Preparation</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHkLosbPmy83"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "from math import floor\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import numpy as np\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su_e3s11m7pm",
        "outputId": "8a74aeca-9f85-47fb-aa0a-f507c0fbbf39"
      },
      "source": [
        "data = arff.loadarff(\"kc2.arff\")\n",
        "df = pd.DataFrame(data[0])\n",
        "df['problems'] = df['problems'].apply(lambda x: x.decode(\"utf-8\"))\n",
        "df['problems'] = df['problems'].map({\"no\": 0, \"yes\": 1})\n",
        "df['problems']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "517    1\n",
              "518    1\n",
              "519    1\n",
              "520    1\n",
              "521    1\n",
              "Name: problems, Length: 522, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJaEiMlqXd5c"
      },
      "source": [
        "def split_db_pos_neg(df):\n",
        "  df = shuffle(df)\n",
        "  negative_database = df.loc[df['problems'] <= 0]\n",
        "  positive_database = df.loc[df['problems'] > 0]\n",
        "  return negative_database,positive_database"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhuXIw0Z8pUt"
      },
      "source": [
        "train Base at 30%:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHAS5FaCoBZP"
      },
      "source": [
        "def train_test_30(negative_database,positive_database,train_array_size):\n",
        "  train_30 = negative_database[0:train_array_sizes[0]]\n",
        "  test_30 = pd.concat([negative_database[train_array_sizes[0]:],positive_database])\n",
        "  train_y_30 = train_30['problems'].values\n",
        "  train_x_30 = train_30.drop(columns=['problems']).values\n",
        "  test_y_30 = test_30['problems'].values\n",
        "  test_x_30 = test_30.drop(columns=['problems']).values\n",
        "  return train_x_30,test_x_30,test_y_30"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7K3jlUx9EnI"
      },
      "source": [
        "train Base at 40%:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-WSUjHA9dL2"
      },
      "source": [
        "def train_test_40(negative_database,positive_database,train_array_size):\n",
        "  train_40 = negative_database[0:train_array_sizes[1]]\n",
        "  test_40 = pd.concat([negative_database[train_array_sizes[1]:],positive_database])\n",
        "  train_y_40 = train_40['problems'].values\n",
        "  train_x_40 = train_40.drop(columns=['problems']).values\n",
        "  test_y_40 = test_40['problems'].values\n",
        "  test_x_40 = test_40.drop(columns=['problems']).values\n",
        "  return train_x_40,test_x_40,test_y_40"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI8jSO8H9gx_"
      },
      "source": [
        "train Base at 50%:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qNC40AZ9wun"
      },
      "source": [
        "def train_test_50(negative_database,positive_database,train_array_size):\n",
        "  train_50 = negative_database[0:train_array_sizes[2]]\n",
        "  test_50 = pd.concat([negative_database[train_array_sizes[2]:],positive_database])\n",
        "  train_y_50 = train_50['problems'].values\n",
        "  train_x_50 = train_50.drop(columns=['problems']).values\n",
        "  test_y_50 = test_50['problems'].values\n",
        "  test_x_50 = test_50.drop(columns=['problems']).values\n",
        "  return train_x_50,test_x_50,test_y_50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A7UBJqP4JSw"
      },
      "source": [
        "train_x_30,test_x_30,test_y_30 = train_test_30(negative_database,positive_database,train_array_sizes)\n",
        "train_x_40,test_x_40,test_y_40 = train_test_40(negative_database,positive_database,train_array_sizes)\n",
        "train_x_50,test_x_50,test_y_50 = train_test_40(negative_database,positive_database,train_array_sizes)\n",
        "\n",
        "train_test_list = [(train_x_30,test_x_30,test_y_30),(train_x_40,test_x_40,test_y_40),(train_x_50,test_x_50,test_y_50)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VE60xEu93Om"
      },
      "source": [
        "<h2><b>One-class-classification Algorithms</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRoNw4cAUTrm"
      },
      "source": [
        "def execute_benchmark(model,negative_database,positive_database,train_array_sizes):   ## Função para benchmark dos modelos\n",
        "  f1_score_list_30 = []\n",
        "  f1_score_list_40 = []\n",
        "  f1_score_list_50 = []\n",
        "  for exec in range(0,100):\n",
        "    train_x_30,test_x_30,test_y_30 = train_test_30(negative_database,positive_database,train_array_sizes)\n",
        "    train_x_40,test_x_40,test_y_40 = train_test_40(negative_database,positive_database,train_array_sizes)\n",
        "    train_x_50,test_x_50,test_y_50 = train_test_40(negative_database,positive_database,train_array_sizes)\n",
        "    train_test_list = [(train_x_30,test_x_30,test_y_30,'30'),(train_x_40,test_x_40,test_y_40,'40'),(train_x_50,test_x_50,test_y_50,'50')]\n",
        "    for i in train_test_list:\n",
        "      train_x, test_x, test_y, label = i\n",
        "      model.fit(train_x)\n",
        "      yhat = model.predict(test_x)\n",
        "      test_y[test_y == 1] = -1\n",
        "      test_y[test_y == 0] = 1\n",
        "      score = f1_score(test_y,yhat,pos_label=-1)\n",
        "      #print('F1 Score for {}: {}; TN = {}, FP = {}, FN = {}, TP = {}'.format(label,score, tn, fp, fn, tp))\n",
        "      if label == '30':\n",
        "        f1_score_list_30.append(score)\n",
        "      elif label == '40':\n",
        "        f1_score_list_40.append(score)\n",
        "      else:\n",
        "        f1_score_list_50.append(score)\n",
        "  print(\"F1-Score 30%: {} (+/- {})\".format(np.mean(f1_score_list_30),np.std(f1_score_list_30)))\n",
        "  print(\"F1-Score 40%: {} (+/- {})\".format(np.mean(f1_score_list_40),np.std(f1_score_list_40)))\n",
        "  print(\"F1-Score 50%: {} (+/- {})\".format(np.mean(f1_score_list_50),np.std(f1_score_list_50)))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT5H0AUk-F4a"
      },
      "source": [
        "<h3><b>Minimum Covariance Determinant:</b></h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CJtIOrct398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f688e0-cb4f-4ff8-fee4-ee97c04a2d5c"
      },
      "source": [
        "from sklearn.covariance import EllipticEnvelope\n",
        "model_mcd = EllipticEnvelope(contamination=0.20)\n",
        "negative_database,positive_database = split_db_pos_neg(df)\n",
        "train_array_sizes = [floor(negative_database.shape[0]*0.3),floor(negative_database.shape[0]*0.4),floor(negative_database.shape[0]*0.5)]\n",
        "execute_benchmark(model_mcd,negative_database,positive_database,train_array_sizes)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/covariance/_robust_covariance.py:170: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-92.323490184346753 > -94.447019250787179). You may want to try with a higher value of support_fraction (current value: 0.566).\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/covariance/_robust_covariance.py:170: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-69.848701255291473 > -88.356824622860287). You may want to try with a higher value of support_fraction (current value: 0.566).\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/covariance/_robust_covariance.py:170: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-70.237456468633241 > -88.265019948273761). You may want to try with a higher value of support_fraction (current value: 0.566).\n",
            "  RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score 30%: 0.6357317027506388 (+/- 0.011783069188804063)\n",
            "F1-Score 40%: 0.6601501473966777 (+/- 0.010628297416583657)\n",
            "F1-Score 50%: 0.6615455010865587 (+/- 0.012180878367950624)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnpVb8HUIpZ8"
      },
      "source": [
        "<h3><b>Local Outlier Factor:</b></h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF4zJybXI6n1"
      },
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from numpy import vstack\n",
        "model_lof = LocalOutlierFactor(contamination=0.20)\n",
        "negative_database,positive_database = split_db_pos_neg(df)\n",
        "train_array_sizes = [floor(negative_database.shape[0]*0.3),floor(negative_database.shape[0]*0.4),floor(negative_database.shape[0]*0.5)]\n",
        "#execute_benchmark(model_lof)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6vCYOcLIxDX",
        "outputId": "9c47b2c5-8089-4735-dccc-b929405a3ef9"
      },
      "source": [
        "f1_score_list_30 = []  # função de benchmark adaptada para Local Outlier Factor\n",
        "f1_score_list_40 = []\n",
        "f1_score_list_50 = []\n",
        "for exec in range(0,100):\n",
        "  train_x_30,test_x_30,test_y_30 = train_test_30(negative_database,positive_database,train_array_sizes)\n",
        "  train_x_40,test_x_40,test_y_40 = train_test_40(negative_database,positive_database,train_array_sizes)\n",
        "  train_x_50,test_x_50,test_y_50 = train_test_40(negative_database,positive_database,train_array_sizes)\n",
        "  train_test_list = [(train_x_30,test_x_30,test_y_30,'30'),(train_x_40,test_x_40,test_y_40,'40'),(train_x_50,test_x_50,test_y_50,'50')]\n",
        "  for i in train_test_list:\n",
        "    train_x, test_x, test_y,label = i\n",
        "    composite = vstack((train_x, test_x))\n",
        "    yhat = model_lof.fit_predict(composite)\n",
        "    # get just the predictions on the test set\n",
        "    yhat = yhat[len(train_x):]\n",
        "\n",
        "    test_y[test_y == 1] = -1\n",
        "    test_y[test_y == 0] = 1\n",
        "    score = f1_score(test_y,yhat,pos_label=-1)\n",
        "    if label == '30':\n",
        "      f1_score_list_30.append(score)\n",
        "    elif label == '40':\n",
        "      f1_score_list_40.append(score)\n",
        "    else:\n",
        "      f1_score_list_50.append(score)\n",
        "print(\"F1-Score 30%: {} (+/- {})\".format(np.mean(f1_score_list_30),np.std(f1_score_list_30)))\n",
        "print(\"F1-Score 40%: {} (+/- {})\".format(np.mean(f1_score_list_40),np.std(f1_score_list_40)))\n",
        "print(\"F1-Score 50%: {} (+/- {})\".format(np.mean(f1_score_list_50),np.std(f1_score_list_50)))\n",
        "    "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-Score 30%: 0.3052631578947368 (+/- 0.0)\n",
            "F1-Score 40%: 0.3204419889502762 (+/- 5.551115123125783e-17)\n",
            "F1-Score 50%: 0.3204419889502762 (+/- 5.551115123125783e-17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnMjcYisKgi2"
      },
      "source": [
        "<h3><b>Isolation Forest:</b></h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UvEgrvtLnj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b2b8dc-491f-409c-e2d3-798725ec514a"
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "model_IF = IsolationForest(contamination=0.20)\n",
        "negative_database,positive_database = split_db_pos_neg(df)\n",
        "train_array_sizes = [floor(negative_database.shape[0]*0.3),floor(negative_database.shape[0]*0.4),floor(negative_database.shape[0]*0.5)]\n",
        "execute_benchmark(model_IF,negative_database,positive_database,train_array_sizes)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-Score 30%: 0.636876322995836 (+/- 0.008493551153021636)\n",
            "F1-Score 40%: 0.6719153658030534 (+/- 0.0100465291201536)\n",
            "F1-Score 50%: 0.671511341544915 (+/- 0.00901067614664682)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxIMiwajMQB6"
      },
      "source": [
        "<h3><b>One-Class Support Vector Machines:</b></h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_C8Gk2JMb7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b96d22-73ac-429d-d7fd-c179f32fd2f1"
      },
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "model_OCSVM = OneClassSVM(gamma='scale', nu=0.20)\n",
        "negative_database,positive_database = split_db_pos_neg(df)\n",
        "train_array_sizes = [floor(negative_database.shape[0]*0.3),floor(negative_database.shape[0]*0.4),floor(negative_database.shape[0]*0.5)]\n",
        "execute_benchmark(model_OCSVM,negative_database,positive_database,train_array_sizes)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-Score 30%: 0.5957446808510639 (+/- 0.0)\n",
            "F1-Score 40%: 0.6244343891402712 (+/- 2.220446049250313e-16)\n",
            "F1-Score 50%: 0.6244343891402712 (+/- 2.220446049250313e-16)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}